{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eb19bc-3619-4fc4-b390-0b17848ed78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics with Linear Regression\n",
      "\n",
      "R2 Score: 0.596054650433006\n",
      "Mean Absolute Error: 0.528942661428345\n",
      "Mean Squared Error: 0.5293336127912477\n",
      "Root Mean Squared Error: 0.727553168360394\n",
      "\n",
      "\n",
      "\n",
      "Metrics with Scaling(Linear Regression)\n",
      "\n",
      "R2 Score: 0.575787706032451\n",
      "Mean Absolute Error: 0.5332001304956562\n",
      "Mean Squared Error: 0.5558915986952442\n",
      "Root Mean Squared Error: 0.727553168360394\n",
      "\n",
      "\n",
      "\n",
      "Metrics with Decision tree \n",
      "\n",
      "R2 Score: 0.5997321244428706\n",
      "Mean Absolute Error: 0.5222592972077786\n",
      "Mean Squared Error: 0.5245146178314735\n",
      "Root Mean Squared Error: 0.727553168360394\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "#fetch the Housing data\n",
    "housing = fetch_california_housing(as_frame = True)\n",
    "\n",
    "#set Features(X) and Target(y)\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "#Combine into one dataframe\n",
    "df = pd.concat([X, y], axis = 1)\n",
    "\n",
    "#Split the data into 80% for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "#fit on training data\n",
    "model.fit(X_test, y_test)\n",
    "\n",
    "#predict y on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Compare the actual test values and the model predictions\n",
    "comparisons = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred})\n",
    "\n",
    "#Model metrics with plain linear regression\n",
    "print(\"Metrics with Linear Regression\\n\")\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2_l = r2_score(y_test, y_pred)\n",
    "print(f\"R2 Score: {r2_l}\\nMean Absolute Error: {mae}\\nMean Squared Error: {mse}\\nRoot Mean Squared Error: {rmse}\\n\\n\\n\")\n",
    "\n",
    "#using scaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "#train\n",
    "model_scaled = make_pipeline(scaler, LinearRegression())\n",
    "model_scaled.fit(X_train, y_train)\n",
    "#predict\n",
    "y_pred_scaled = model_scaled.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#Model metrics with scaling\n",
    "print(\"Metrics with Scaling(Linear Regression)\\n\")\n",
    "mae_scaled = mean_absolute_error(y_test, y_pred_scaled)\n",
    "mse_scaled = mean_squared_error(y_test, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse)\n",
    "r2_scaled = r2_score(y_test, y_pred_scaled)\n",
    "print(f\"R2 Score: {r2_scaled}\\nMean Absolute Error: {mae_scaled}\\nMean Squared Error: {mse_scaled}\\nRoot Mean Squared Error: {rmse_scaled}\\n\\n\\n\")\n",
    "\n",
    "\n",
    "#Using Decision Tree\n",
    "tree_model = DecisionTreeRegressor(max_depth = 5, random_state=42)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_tree = tree_model.predict(X_test)\n",
    "\n",
    "#Model metrics with decision trees\n",
    "print(\"Metrics with Decision tree \\n\")\n",
    "mae_tree = mean_absolute_error(y_test, y_pred_tree)\n",
    "mse_tree = mean_squared_error(y_test, y_pred_tree)\n",
    "rmse_tree = np.sqrt(mse)\n",
    "r2_tree = r2_score(y_test, y_pred_tree)\n",
    "print(f\"R2 Score: {r2_tree}\\nMean Absolute Error: {mae_tree}\\nMean Squared Error: {mse_tree}\\nRoot Mean Squared Error: {rmse_tree}\\n\\n\\n\")\n",
    "\n",
    "#Using Random Forest\n",
    "forest_model = RandomForestRegressor(n_estimators = 500, min_samples_split =  2, min_samples_leaf = 2, max_features = 'log2')\n",
    "forest_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_forest = forest_model.predict(X_test)\n",
    "\n",
    "#Model metrics with Random Forest\n",
    "print(\"Metrics with Random Forest \\n\")\n",
    "mae_forest = mean_absolute_error(y_test, y_pred_forest)\n",
    "mse_forest = mean_squared_error(y_test, y_pred_forest)\n",
    "rmse_forest = np.sqrt(mse)\n",
    "r2_forest = r2_score(y_test, y_pred_forest)\n",
    "print(f\"R2 Score: {r2_forest}\\nMean Absolute Error: {mae_forest}\\nMean Squared Error: {mse_forest}\\nRoot Mean Squared Error: {rmse_forest}\\n\\n\\n\")\n",
    "\n",
    "\n",
    "#Using GradientBoostingRegressor\n",
    "gb_model = GradientBoostingRegressor(learning_rate = 0.1, max_depth = 5, n_estimators = 300, subsample =0.8,random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "\n",
    "#hyperparameter tuning for GradientBoostingRegressor\n",
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [100, 200, 300],\n",
    "    'learning_rate' : [0.01, 0.05, 0.1],\n",
    "    'max_depth' : [3,4,5],\n",
    "    'subsample' : [0.8, 1.0]  \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    gb,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best R2 Score (CV)\", grid_search.best_score_)\n",
    "\n",
    "#Model metrics with Gradient Boosting(with hyper parameters attained through hypeer parameter tuning)\n",
    "print(\"Metrics with Gradient Boosting\\n\")\n",
    "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "rmse_gb = np.sqrt(mse)\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "print(f\"R2 Score: {r2_gb}\\nMean Absolute Error: {mae_gb}\\nMean Squared Error: {mse_gb}\\nRoot Mean Squared Error: {rmse_gb}\\n\\n\\n\\n\\n\\n\\n\")\n",
    "\n",
    "\n",
    "#Using XGBoost\n",
    "# Hyperparameter tuning\n",
    "#defining the model\n",
    "xgb = XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "param_dist = {\n",
    "    \"n_estimators\": [100, 200, 300, 500],\n",
    "    \"max_depth\": [3, 4, 5, 6,8],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0]\n",
    "}\n",
    "#RandomizedSearchCV setup\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimate=xgb,\n",
    "    param_distribution = param_dist,\n",
    "    n_iter = 20,\n",
    "    cv=3,\n",
    "    scoring=\"r2\",\n",
    "    andom_state=42,\n",
    "    n_jobs = -1,\n",
    "    verbose=2\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "#best params\n",
    "print(\"Best Parameters: \\n\\n\\n\\n\\n\", random_search.best_params_)\n",
    "\n",
    "\n",
    "#Evaluate the tuned model\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "y_pred_xgb_best = best_xgb_model.predict(X_test)\n",
    "\n",
    "#initializing extreme gradient boosting\n",
    "\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state =42\n",
    ")\n",
    "#Train\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "#predict\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "#Model metrics with XGBoost\n",
    "print(\"Metrics with Extreme Gradient Boosting\\n\")\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "rmse_xgb = np.sqrt(mse)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "print(f\"R2 Score: {r2_xgb}\\nMean Absolute Error: {mae_xgb}\\nMean Squared Error: {mse_xgb}\\nRoot Mean Squared Error: {rmse_xgb}\\n\\n\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378da96b-6128-4715-910d-e2c030a3070b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
